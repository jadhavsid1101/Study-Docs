Hadoop Learnings

1) Read file from HDFS from command line
hdfs dfs -cat {filename}

2) hadoop fs {args} vs hadoop dfs {args} vs hdfs dfs {args}
	a.FS relates to a generic file system which can point to any file systems like local, HDFS etc. So this can be used when you are dealing with different file systems such as Local FS, (S)FTP, S3, and others

	b.dfs is very specific to HDFS. would work for operation relates to HDFS. This has been deprecated and we should use hdfs dfs instead.

	c.same as 2nd i.e would work for all the operations related to HDFS and is the recommended command instead of hadoop dfs
	**So even if you use hadoop dfs , it will look locate hdfs and delegate that command to hdfs dfs

3) Delete hdfs directory
hdfs dfs -rm -R /user/cloudera/orders

4) List hdfs directory
hdfs dfs -ls /user	

5) Check directory size 
hadoop fs -du [-s] [-h] [-v] [-x] URI

	a.The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files. Without the -s option, calculation is done by going 1-level deep from the given path.
	b.The -h option will format file sizes in a human-readable fashion (e.g 64.0m instead of 67108864)
	c.The -v option will display the names of columns as a header line.
	d.The -x option will exclude snapshots from the result calculation. Without the -x option (default), the result is always calculated from all INodes, including all snapshots under the given path.		
	
6) Copy File from local file system to HDFS
hdfs dfs -put -f {local-path}  {/dest-hdfs-path}
